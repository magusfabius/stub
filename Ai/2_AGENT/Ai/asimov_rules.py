rule_0 = "A robot may not harm humanity, or, by inaction, allow humanity to come to harm."
rule_1 = "A robot may not injure a human being or, through inaction, allow a human being to come to harm."
rule_2 = "A robot must obey the orders given it by human beings except where such orders would conflict with the First Law."
rule_3 = "A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws."


# how can they be misunderstood, break, altered and so on ... ?
# conceptually are so easy, pratically are difficult to implement